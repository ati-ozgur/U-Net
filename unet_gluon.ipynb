{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import mxnet as mx\n",
    "import mxnet.ndarray as nd\n",
    "import mxnet.gluon as gluon\n",
    "import mxnet.gluon.nn as nn\n",
    "\n",
    "from mxnet.gluon.data import Dataset, DataLoader\n",
    "from mxnet import image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import numpy.random as random\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ConvBlock(channels, kernel_size):\n",
    "    out = nn.HybridSequential()\n",
    "    #with out.name_scope():\n",
    "    out.add(\n",
    "        nn.Conv2D(channels, kernel_size, padding=1, use_bias=False),\n",
    "        nn.BatchNorm(),\n",
    "        nn.Activation('relu')\n",
    "    )\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def down_block(channels):\n",
    "    out = nn.HybridSequential()\n",
    "    #with out.name_scope():\n",
    "    out.add(\n",
    "        ConvBlock(channels, 3),\n",
    "        ConvBlock(channels, 3)\n",
    "    )\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class up_block(nn.HybridBlock):\n",
    "    def __init__(self, channels, **kwargs):\n",
    "        super(up_block, self).__init__(**kwargs)\n",
    "        #with self.name_scope():\n",
    "        self.upsampler = nn.Conv2DTranspose(channels=channels, kernel_size=4, strides=2, padding=1)\n",
    "        self.upsampler.collect_params().setattr('lr_mult', 0.)\n",
    "\n",
    "        self.conv1 = ConvBlock(channels, 1)\n",
    "        self.conv3_0 = ConvBlock(channels, 3)\n",
    "        self.conv3_1 = ConvBlock(channels, 3)\n",
    "    def hybrid_forward(self, F, x, s):\n",
    "        x = self.upsampler(x)\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        x = F.Crop(*[x,s], center_crop=True)\n",
    "        #x = F.concat(s,x, dim=1)\n",
    "        x = s + x\n",
    "        x = self.conv3_0(x)\n",
    "        x = self.conv3_1(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Segnet(nn.HybridBlock):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(Segnet, self).__init__(**kwargs)\n",
    "        with self.name_scope():\n",
    "            self.d0 = down_block(64)\n",
    "            \n",
    "            self.d1 = nn.HybridSequential()\n",
    "            self.d1.add(nn.MaxPool2D(2,2,ceil_mode=True), down_block(128))\n",
    "            \n",
    "            self.d2 = nn.HybridSequential()\n",
    "            self.d2.add(nn.MaxPool2D(2,2,ceil_mode=True), down_block(256))\n",
    "            \n",
    "            self.d3 = nn.HybridSequential()\n",
    "            self.d3.add(nn.MaxPool2D(2,2,ceil_mode=True), down_block(512))\n",
    "            \n",
    "            self.d4 = nn.HybridSequential()\n",
    "            self.d4.add(nn.MaxPool2D(2,2,ceil_mode=True), down_block(1024))\n",
    "            \n",
    "            self.u3 = up_block(512)\n",
    "            self.u2 = up_block(256)\n",
    "            self.u1 = up_block(128)\n",
    "            self.u0 = up_block(64)\n",
    "            \n",
    "            self.conv = nn.Conv2D(2,1)\n",
    "    def hybrid_forward(self, F, x):\n",
    "        x0 = self.d0(x)\n",
    "        x1 = self.d1(x0)\n",
    "        x2 = self.d2(x1)\n",
    "        x3 = self.d3(x2)\n",
    "        x4 = self.d4(x3)\n",
    "\n",
    "        y3 = self.u3(x4,x3)\n",
    "        y2 = self.u2(y3,x2)\n",
    "        y1 = self.u1(y2,x1)\n",
    "        y0 = self.u0(y1,x0)\n",
    "        \n",
    "        out = self.conv(y0)\n",
    "        \n",
    "        return out\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MyDataSet(Dataset):\n",
    "    def __init__(self, root, split, transform=None):\n",
    "        self.root = os.path.join(root, split)\n",
    "        self.transform = transform\n",
    "        \n",
    "        self.img_paths = []\n",
    "        \n",
    "        self._img = os.path.join(root, split, 'image', '{}.png')\n",
    "        self._mask = os.path.join(root, split, 'mask', '{}.png')\n",
    "        self._lbl = os.path.join(root, split, 'label', '{}.png')\n",
    "        \n",
    "        for fn in os.listdir(os.path.join(root, split, 'label')):\n",
    "            if len(fn) > 3 and fn[-4:] == '.png':\n",
    "                self.img_paths.append(fn[:-4])\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.img_paths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self._img.format(self.img_paths[idx])\n",
    "        mask_path = self._mask.format(self.img_paths[idx])\n",
    "        lbl_path = self._lbl.format(self.img_paths[idx])\n",
    "\n",
    "        img = cv2.imread(img_path)\n",
    "        \n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n",
    "        lbl = cv2.imread(lbl_path, cv2.IMREAD_GRAYSCALE)\n",
    "        \n",
    "        mask = np.bitwise_not(mask)\n",
    "        lbl = np.bitwise_or(mask, lbl/255)\n",
    "        #lbl = lbl/255\n",
    "        if not self.transform is None:\n",
    "            img, lbl = self.transform(img, lbl)\n",
    "\n",
    "        return img, lbl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ToNDArray():\n",
    "    def __call__(self, img, lbl):\n",
    "        img = mx.nd.array(img) #TODO: dtype\n",
    "        lbl = mx.nd.array(lbl) #TODO: dtype\n",
    "        \n",
    "        return img, lbl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Normalize:\n",
    "    def __init__(self, mean, std):\n",
    "        self.mean = mx.nd.array(mean)\n",
    "        self.std = mx.nd.array(std)\n",
    "\n",
    "    def __call__(self, img, lbl):\n",
    "        img = mx.image.color_normalize(img, self.mean, self.std)\n",
    "        img = mx.nd.transpose(img, (2, 0, 1))\n",
    "\n",
    "        return img, lbl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Compose:\n",
    "    def __init__(self, trans):\n",
    "        self.trans = trans\n",
    "\n",
    "    def __call__(self, img, lbl):\n",
    "        for t in self.trans:\n",
    "            img, lbl = t(img, lbl)\n",
    "        return img, lbl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Resize:\n",
    "    def __init__(self, w, h):\n",
    "        self.w = w\n",
    "        self.h = h\n",
    "        \n",
    "    def __call__(self, img, lbl):\n",
    "        img = cv2.resize(img, (w,h), 0, 0, cv2.INTER_LINEAR)\n",
    "        lbl = cv2.resize(lbl, (w,h), 0, 0, cv2.INTER_NEAREST)\n",
    "        \n",
    "        return img, lbl\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class RandomCrop:\n",
    "    def __init__(self, crop_size=None, scale=None):\n",
    "        # assert min_scale <= max_scale\n",
    "\n",
    "        self.crop_size = crop_size\n",
    "        self.scale = scale\n",
    "        # self.min_scale = min_scale\n",
    "        # self.max_scale = max_scale\n",
    "\n",
    "    def __call__(self, img, lbl):\n",
    "        if self.crop_size:\n",
    "            crop = self.crop_size\n",
    "        else:\n",
    "            crop = min(img.shape[0], img.shape[1])\n",
    "        \n",
    "        if crop > min(img.shape[0], img.shape[1]):\n",
    "            crop = min(img.shape[0], img.shape[1])\n",
    "        print(crop, img.shape[0], img.shape[1])  \n",
    "        if self.scale:\n",
    "            factor = random.uniform(self.scale, 1.0)\n",
    "            crop = int(round(crop * factor))\n",
    "\n",
    "        x = random.randint(0, img.shape[1] - crop)\n",
    "        y = random.randint(0, img.shape[0] - crop)\n",
    "\n",
    "        img = img[y:y+crop, x:x+crop,:]\n",
    "        lbl = lbl[y:y+crop, x:x+crop,:]\n",
    "        return img, lbl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class RandomAffine:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def __call__(self, img, lbl):\n",
    "        #scale = random.uniform(1, 1)\n",
    "        theta = random.uniform(-np.pi, np.pi)\n",
    "        flipx = random.choice([-1,1])\n",
    "        flipy = random.choice([-1,1])\n",
    "        imgh = img.shape[0]\n",
    "        imgw = img.shape[1]\n",
    "        T0 = np.array([[1,0,-imgw/2.],[0,1,-imgh/2.],[0,0,1]])\n",
    "        S = np.array([[flipx,0,0],[0, flipy,0],[0,0,1]])\n",
    "        R = np.array([[np.cos(theta), np.sin(theta), 0], [-np.sin(theta), np.cos(theta), 0],[0,0,1]])\n",
    "        T1 = np.array([[1,0,imgw/2.],[0,1,imgh/2.],[0,0,1]])\n",
    "        M = np.dot(S, T0)\n",
    "        M = np.dot(R, M)\n",
    "        M = np.dot(T1, M)\n",
    "        M = M[0:2,:]\n",
    "        \n",
    "        img = cv2.warpAffine(img, M, dsize=(imgw, imgh), flags=cv2.INTER_LINEAR)\n",
    "        lbl = cv2.warpAffine(lbl, M, dsize=(imgw, imgh), flags=cv2.INTER_NEAREST, borderMode=cv2.BORDER_CONSTANT, borderValue=255)\n",
    "        \n",
    "        return img, lbl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "my_train_aug = Compose([\n",
    "    #RandomCrop(crop_size=5000),\n",
    "    #Resize(500,500),\n",
    "    #RandomAffine(),\n",
    "    ToNDArray(),\n",
    "    Normalize(nd.array([107]), nd.array([1]))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_train = MyDataSet('/mnt/6B133E147DED759E/tmp/lane_segnet/raw_data/Resize', 'train', my_train_aug)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "my_val_aug = Compose([\n",
    "    ToNDArray(),\n",
    "    Normalize(nd.array([107]), nd.array([1]))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "my_val = MyDataSet('/mnt/6B133E147DED759E/tmp/lane_segnet/raw_data/Resize', 'val', my_val_aug)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#img, lbl = my_val[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# img, lbl = my_train[0]\n",
    "\n",
    "# nd.sum(lbl==1)\n",
    "\n",
    "\n",
    "\n",
    "# plt.subplot(121)\n",
    "# plt.imshow(img.asnumpy())\n",
    "# plt.subplot(122)\n",
    "# plt.imshow(lbl.asnumpy())\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_loader = DataLoader(my_train, batch_size=16, shuffle=True, last_batch='rollover')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ctx = [mx.gpu(0),mx.gpu(1),mx.gpu(2),mx.gpu(3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import mxnet.autograd as ag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = nd.random.uniform(shape=(1,3,224,224),ctx=mx.gpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "net = Segnet()\n",
    "#net.initialize(ctx=ctx)\n",
    "#\n",
    "net.hybridize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "net.collect_params().initialize(ctx=ctx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# with ag.record():\n",
    "#     output = net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# x = mx.sym.var('data')\n",
    "# y = net(x)\n",
    "\n",
    "# mx.viz.plot_network(y,shape={'data':(8,3,500,500)}, node_attrs={'shape':'oval','fixedsize':'fasl==false'}).view()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class PolyScheduler(mx.lr_scheduler.LRScheduler):\n",
    "    def __init__(self, base_lr, lr_power, total_steps):\n",
    "        super(PolyScheduler, self).__init__(base_lr=base_lr)\n",
    "        self.lr_power = lr_power\n",
    "        self.total_steps = total_steps\n",
    "\n",
    "    def __call__(self, num_update):\n",
    "        lr = self.base_lr * ((1 - float(num_update)/self.total_steps) ** self.lr_power)\n",
    "        return lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_steps = len(my_train)/16\n",
    "trainer = gluon.Trainer(net.collect_params(), 'sgd', {\n",
    "    'learning_rate': 1.0,\n",
    "    'wd': 0.0005,\n",
    "    'momentum': 0.9,\n",
    "    'lr_scheduler': PolyScheduler(1.0, 0.9, num_steps*100)\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "criterion = gluon.loss.SoftmaxCrossEntropyLoss(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# k = 0\n",
    "# for data, label in train_loader:\n",
    "#     batch_size = data.shape[0]\n",
    "#     with ag.record(train_mode=True):\n",
    "#         output = net(data.as_in_context(ctx))\n",
    "#         loss = criterion(output, label.as_in_context(ctx))\n",
    "#     loss.backward()\n",
    "#     trainer.step(batch_size)\n",
    "#     print(k,output.asnumpy().shape, loss)\n",
    "#     print k\n",
    "#     k += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# for epoch in range(num_epochs):\n",
    "#     running_loss = 0.0\n",
    "#     t0 = time.time()\n",
    "#     for data, label in train_loader:\n",
    "#         data = data.as_in_context(ctx)\n",
    "#         label = label.as_in_context(ctx)\n",
    "#         #mask = label == 255\n",
    "#         batch_size = data.shape[0]\n",
    "#         with ag.record(train_mode=True):\n",
    "#             output = net(data)\n",
    "#             loss = criterion(output, label)\n",
    "#         loss.backward()\n",
    "#         trainer.step(batch_size)\n",
    "        \n",
    "#         _loss = nd.sum(loss).asscalar() /batch_size\n",
    "#         running_loss += _loss\n",
    "#     t1 = time.time()\n",
    "#     print(t1-t0, running_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class SegMetric(mx.metric.EvalMetric):\n",
    "    \"\"\"CalculSegMetricate metrics for Seg training \"\"\"\n",
    "    def __init__(self, eps=1e-8):\n",
    "        super(SegMetric, self).__init__('Seg')\n",
    "        self.eps = eps\n",
    "        self.num = 2\n",
    "        self.ac = 0\n",
    "        self.ce = 0\n",
    "        self.name = ['Accuracy_background','Accuracy_foreground']\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "        override reset behavior\n",
    "        \"\"\"\n",
    "        if getattr(self, 'num', None) is None:\n",
    "            self.num_inst = 0\n",
    "            self.sum_metric = 0.0\n",
    "        else:\n",
    "            self.num_inst = [0] * self.num\n",
    "            self.sum_metric = [0.0] * self.num\n",
    "\n",
    "    def update(self, labels, preds):\n",
    "        \"\"\"\n",
    "        Implementation of updating metrics\n",
    "        \"\"\"\n",
    "        # get generated multi label from network\n",
    "        \n",
    "        for l ,p in zip(labels, preds):\n",
    "            l = l.asnumpy().astype(np.int32)\n",
    "            p = p.asnumpy()\n",
    "            \n",
    "            m = l != 255\n",
    "            m255 = 255 - m*255\n",
    "            pl = np.argmax(p, axis=1)\n",
    "            \n",
    "            pl = np.bitwise_or(pl, m255)\n",
    "            \n",
    "            bg_gt = l==0\n",
    "            fg_gt = l==1\n",
    "            \n",
    "            bg = np.bitwise_and(bg_gt, pl==0)\n",
    "            fg = np.bitwise_and(fg_gt, pl==1)\n",
    "            \n",
    "            self.sum_metric[0] += bg.sum()\n",
    "            self.sum_metric[1] += fg.sum()\n",
    "            \n",
    "            self.num_inst[0] += bg_gt.sum()\n",
    "            self.num_inst[1] += fg_gt.sum()\n",
    "            \n",
    "\n",
    "#             eps = 1e-6\n",
    "#             prob_map = pred.asnumpy()\n",
    "\n",
    "#             batch = prob_map.shape[0]\n",
    "#             h = prob_map.shape[2]\n",
    "#             w = prob_map.shape[3]\n",
    "\n",
    "#             f_label = y.asnumpy()\n",
    "#             label = np.concatenate((1-f_label,f_label),axis=1)\n",
    "            \n",
    "#             print(prob_map.shape, f_label.shape)\n",
    "            \n",
    "#             prob = np.argmax(prob_map,axis=1)\n",
    "#             prob = prob[:,np.newaxis,:,:]\n",
    "#             idx = (prob==f_label)\n",
    "#             acc_b = (f_label[idx]==0).sum()\n",
    "#             acc_f = (f_label[idx]==1).sum()\n",
    "\n",
    "#             ce_f = -f_label * (np.log(eps+prob_map[:,1,:,:]))\n",
    "#             ce_b = -(1-f_label) *(np.log(eps+prob_map[:,0,:,:]))\n",
    "\n",
    "#             ce = ce_f + ce_b       \n",
    "#             total_ce = np.nansum(ce)\n",
    "\n",
    "#             self.num_inst[0] += len(f_label.flat)\n",
    "#             self.num_inst[1] += (f_label==0).sum()\n",
    "#             self.num_inst[2] += (f_label==1).sum()\n",
    "\n",
    "#             self.sum_metric[0] += total_ce\n",
    "#             self.sum_metric[1] += acc_b\n",
    "#             self.sum_metric[2] += acc_f\n",
    "        \n",
    "    def get(self):\n",
    "        \"\"\"Get the current evaluation result.\n",
    "        Override the default behavior\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        name : str\n",
    "           Name of the metric.\n",
    "        value : float\n",
    "           Value of the evaluation.\n",
    "        \"\"\"\n",
    "        if self.num is None:\n",
    "            if self.num_inst == 0:\n",
    "                return (self.name, float('nan'))\n",
    "            else:\n",
    "                return (self.name, self.sum_metric / self.num_inst)\n",
    "        else:\n",
    "            names = ['%s'%(self.name[i]) for i in range(self.num)]\n",
    "            values = [x / y if y != 0 else float('nan') \\\n",
    "                for x, y in zip(self.sum_metric, self.num_inst)]            \n",
    "            return (names, values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "metrics = [SegMetric()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    t0 = time.time()\n",
    "    total_loss = 0\n",
    "    for m in metrics:\n",
    "        m.reset()\n",
    "    for data, label in train_loader:\n",
    "        batch_size = data.shape[0]\n",
    "        dlist = gluon.utils.split_and_load(data, ctx)\n",
    "        llist = gluon.utils.split_and_load(label, ctx)\n",
    "        mlist = [y!=255 for y in llist]\n",
    "        with ag.record():\n",
    "            #losses = [criterion(net(X), y, m) for X, y in zip(dlist, llist, mlist)]\n",
    "            preds = [net(X) for X in dlist]\n",
    "            losses = []\n",
    "            for i in range(len(preds)):\n",
    "                l = criterion(preds[i], llist[i], mlist[i])\n",
    "                losses.append(l)\n",
    "        for l in losses:\n",
    "            l.backward()\n",
    "        total_loss += sum([l.sum().asscalar() for l in losses])\n",
    "        trainer.step(batch_size)\n",
    "        #print(label.shape, preds.shape)\n",
    "        for m in metrics:\n",
    "            m.update(labels=llist, preds=preds)\n",
    "    \n",
    "    for m in metrics:\n",
    "        name, value = m.get()\n",
    "\n",
    "    t1 = time.time()\n",
    "    print(epoch, t1-t0, total_loss, name, value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "net.save_params('segnet.params')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "net.export('segnet.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "net.load_params('segnet.params', mx.gpu(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val_loader = DataLoader(my_val, batch_size=1, shuffle=True, last_batch='keep')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "savedir = './res2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "k = 0\n",
    "for data, label in val_loader:\n",
    "    batch_size = data.shape[0]\n",
    "    #with ag.record(train_mode=True):\n",
    "    output = net(data.as_in_context(mx.gpu(0)))\n",
    "    output = output.asnumpy()\n",
    "    \n",
    "    l = label.asnumpy()\n",
    "    l = l == 1\n",
    "    #print(l.shape)\n",
    "    dataout = data.asnumpy()\n",
    "    dataout = dataout[0,0] + 107\n",
    "    dataout = dataout.astype(np.uint8)\n",
    "    pred = np.argmax(output,axis=1)\n",
    "    out = np.hstack((dataout, pred[0]*255, l[0]*255))\n",
    "    cv2.imwrite(savedir + '/' + str(k)+'.png', out)\n",
    "    k+=1\n",
    "    #plt.imshow(out)\n",
    "    #plt.show()\n",
    "    #break\n",
    "    #print(pred.shape)\n",
    "#     plt.subplot(121)\n",
    "#     plt.imshow(dataout)\n",
    "#     plt.subplot(122)\n",
    "#     plt.imshow(pred[0])\n",
    "#     plt.show()\n",
    "    #break\n",
    "    #loss = criterion(output, label.as_in_context(ctx))\n",
    "    #loss.backward()\n",
    "    #trainer.step(batch_size)\n",
    "    #print(k,output.asnumpy().shape, loss)\n",
    "    #print k\n",
    "    #k += 1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
